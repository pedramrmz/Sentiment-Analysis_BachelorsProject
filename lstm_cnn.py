# -*- coding: utf-8 -*-
"""lstm3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1awhanYQnpB7s9QobEKRLDtZLTrGhTst9
"""

!pip install tensorflow
!pip install hazm
!pip install urlextract
!pip install emojis
!pip install keras

# اتصال به گوگل درایو و بارگذاری داده‌ها
from google.colab import drive

# اتصال به گوگل درایو
drive.mount('/content/drive')
file_path = '/content/drive/My Drive/bachlor project/output_augmented.csv'

import re
import emojis
from urlextract import URLExtract
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords

# لیست کلمات توقفی فارسی
stop_words = set([
    'و', 'به', 'از', 'که', 'را', 'این', 'آن', 'با', 'برای', 'در', 'تا', 'پس',
    'شد', 'گفت', 'است', 'نمی', 'هم', 'اگر', 'باشد', 'باشند', 'شده', 'میشود',
    'داشت', 'بوده', 'کرد', 'کردن', 'کرده', 'باشیم', 'کنند'
])

# نقشه تبدیل فینگلیش به فارسی
finglish_to_farsi = {
    'a': 'ا', 'b': 'ب', 'p': 'پ', 't': 'ت', 's': 'س', 'j': 'ج', 'ch': 'چ', 'h': 'ح',
    'kh': 'خ', 'd': 'د', 'z': 'ز', 'r': 'ر', 'zh': 'ژ', 's': 'س', 'sh': 'ش', 's': 'ص',
    'z': 'ض', 't': 'ط', 'z': 'ظ', 'e': 'ع', 'gh': 'غ', 'f': 'ف', 'q': 'ق', 'k': 'ک',
    'g': 'گ', 'l': 'ل', 'm': 'م', 'n': 'ن', 'v': 'و', 'y': 'ی', 'u': 'و', 'oo': 'او',
    'ee': 'ای'
}


# تابع پیش‌پردازش
def preprocess(text):
    def _multiple_replace(mapping, text):
        pattern = "|".join(map(re.escape, mapping.keys()))
        return re.sub(pattern, lambda m: mapping[m.group()], str(text))

    def convert_fa_numbers(input_str):
        mapping = {
            '۰': '0', '۱': '1', '۲': '2', '۳': '3', '۴': '4',
            '۵': '5', '۶': '6', '۷': '7', '۸': '8', '۹': '9',
        }
        return _multiple_replace(mapping, input_str)

    def convert_ar_characters(input_str):
        mapping = {
            'ك': 'ک', 'ى': 'ی', 'ي': 'ی', 'ئ': 'ی', 'إ': 'ا',
            'أ': 'ا', 'ة': 'ه', 'ؤ': 'و'
        }
        return _multiple_replace(mapping, input_str)

    def removeEmojies(text):
        e = ("["
              u"\U0001F600-\U0001F64F"
              u"\U0001F300-\U0001F5FF"
              u"\U0001F680-\U0001F6FF"
              u"\U0001F1E0-\U0001F1FF"
              u"\U00002500-\U00002BEF"
              u"\U00002702-\U000027B0"
              u"\U000024C2-\U0001F251"
              u"\U0001f926-\U0001f937"
              u"\U00010000-\U0010ffff"
              u"\u2640-\u2642"
              u"\u2600-\u2B55"
              u"\u200d"
              u"\u23cf"
              u"\u23e9"
              u"\u231a"
              u"\ufe0f"
              u"\u3030"
              "]+")
        return re.sub(e, '', text)

    def remove_urls(text):
        extractor = URLExtract()
        for url in extractor.gen_urls(text):
            text = text.replace(url, '<URL>')
        return text

    def normalize(text):
        text = re.sub(r'[‌ـ]', '', text)  # حذف نیم‌فاصله‌ها
        text = re.sub(r'[\u064B-\u0652]', '', text)  # حذف علائم فتحه و تنوین
        return text

    def convert_finglish_to_farsi(text):
        # تبدیل فینگلیش به فارسی
        for key, value in finglish_to_farsi.items():
            text = re.sub(rf'\b{key}\b', value, text)
        return text

    # تبدیل فینگلیش به فارسی
    text = convert_finglish_to_farsi(text)

    # تبدیل اعداد فارسی به انگلیسی
    text = convert_fa_numbers(text)
    # تبدیل حروف عربی به فارسی
    text = convert_ar_characters(text)
    # حذف ایموجی‌ها
    text = removeEmojies(text)
    # حذف URL‌ها
    text = remove_urls(text)

    # تبدیل smiley به کلمه '<smiley>'
    text = re.sub(r"(:\)|:-\)|:\(|:-\()", '<smiley>', text)

    # کوچک‌نویسی تمام متن
    text = text.lower()

    # نرمال‌سازی متن
    text = normalize(text)

    # حذف نویسه‌های خاص مانند #، @، .، :، ؟، !، ...
    text = re.sub(r'[<>#.:()"\'!?؟،؛,@$%^&*_+\[\]/]', ' ', text)

    # حذف فاصله‌های اضافی
    text = re.sub(r'[\s]{2,}', ' ', text)

    # حذف تکرار حروف
    text = re.sub(r'(\w)\1{2,}', r'\1', text)

    # حذف کلمات توقفی
    text = ' '.join([word for word in text.split() if word not in stop_words])

    # استفاده از توکنایزر برای جلوگیری از کلمات تکراری مشابه
    text = ' '.join(word_tokenize(text))

    # بررسی اینکه آیا متن شامل نویسه‌های فارسی است یا خیر
    if re.search(r'[\u0600-\u06FF]', text):
        return text.strip()
    else:
        return ''  # اگر شامل نویسه‌های فارسی نیست، متن را خالی برمی‌گرداند

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.callbacks import EarlyStopping
from keras.layers import BatchNormalization
from hazm import Normalizer, word_tokenize, stopwords_list
import re

# بارگذاری دیتاست
corpus = pd.read_csv(file_path, sep=',', on_bad_lines='skip')
corpus.columns = corpus.columns.str.strip()  # حذف فضای خالی از ابتدا و انتهای نام ستون‌ها

# ایجاد شیء Normalizer برای پردازش متن
normalizer = Normalizer()

# تعریف نقشه تبدیل فینگلیش به فارسی (در صورت نیاز)
finglish_to_farsi = {
    'a': 'ا', 'b': 'ب', 'p': 'پ', 't': 'ت', 's': 'س', 'j': 'ج', 'ch': 'چ', 'h': 'ح',
    'kh': 'خ', 'd': 'د', 'z': 'ز', 'r': 'ر', 'zh': 'ژ', 'sh': 'ش', 's': 'ص', 'z': 'ض',
    't': 'ط', 'z': 'ظ', 'e': 'ع', 'gh': 'غ', 'f': 'ف', 'q': 'ق', 'k': 'ک', 'g': 'گ',
    'l': 'ل', 'm': 'م', 'n': 'ن', 'v': 'و', 'y': 'ی', 'u': 'و', 'oo': 'او', 'ee': 'ای'
}

# تعریف تابع برای تبدیل فینگلیش به فارسی
def convert_finglish_to_farsi(text):
    for finglish, farsi in finglish_to_farsi.items():
        text = re.sub(finglish, farsi, text)
    return text

# تعریف تابع پیش‌پردازش متن
def preprocess(text):
    # 1. نرمال‌سازی متن (اصلاح حروف بزرگ و کوچک، اصلاح کاراکترها)
    text = normalizer.normalize(text)

    # 2. تبدیل فینگلیش به فارسی
    text = convert_finglish_to_farsi(text)

    # 3. حذف اعداد
    text = re.sub(r'\d+', '', text)

    # 4. حذف کاراکترهای خاص و اضافی
    text = re.sub(r'[^\w\s]', '', text)

    # 5. توکن‌سازی متن با استفاده از word_tokenize از hazm
    tokens = word_tokenize(text)

    # 6. حذف stopwords
    stop_words = stopwords_list()
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # 7. بازگشت به متن از توکن‌ها
    return " ".join(filtered_tokens)

# پیش‌پردازش متن
corpus['txet'] = corpus['text'].apply(preprocess)

# آماده‌سازی ورودی‌ها و برچسب‌ها
X = corpus['txet']  # متن‌ها
labels = corpus['label']  # برچسب‌ها

# حذف مقادیر گم‌شده
X = X[labels.notnull()]
labels = labels.dropna()


# تصادفی کردن داده‌ها
data = pd.DataFrame({'X': X, 'y': labels})
data = data.sample(frac=1, random_state=180).reset_index(drop=True)

# حالا داده‌ها به طور تصادفی مخلوط شدن
X_shuffled = data['X']
y_shuffled = data['y']

# تقسیم داده‌ها به مجموعه‌های آموزش و آزمون
X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.2, random_state=180)

# توکن‌سازی متون
tokenizer = Tokenizer(num_words=20000, oov_token="<OOV>")  # اضافه کردن توکن برای کلمات خارج از واژگان
tokenizer.fit_on_texts(X_train)

# تبدیل متون به توالی‌های عددی
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# پد کردن توالی‌ها (محدود کردن طول توالی‌ها)
max_length = 1000  # حداکثر طول توالی
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# تبدیل برچسب‌ها به قالب one-hot
y_train_cat = to_categorical(y_train, num_classes=2)
y_test_cat = to_categorical(y_test, num_classes=2)

# نمایش تعداد داده‌های موجود برای آموزش و آزمون
print(f"تعداد داده‌های آموزش: {len(X_train)}, تعداد داده‌های آزمون: {len(X_test)}")

corpus.columns = corpus.columns.str.strip()  # حذف فضای خالی از ابتدا و انتهای نام ستون‌ها
corpus = pd.read_csv(file_path, sep=',', on_bad_lines='skip')
print(corpus.head())  # نمایش اولین 5 ردیف از DataFrame

# شمارش تعداد داده‌های مثبت و منفی
positive_count = labels[labels == 1].count()  # تعداد داده‌های مثبت
negative_count = labels[labels == -1].count()  # تعداد داده‌های منفی

# نمایش تعداد داده‌های مثبت و منفی
print(f"تعداد داده‌های مثبت: {positive_count}")
print(f"تعداد داده‌های منفی: {negative_count}")
# انتخاب تصادفی 10 سطر از داده‌ها
sample_data = pd.DataFrame({'Text': X.sample(10).values, 'Label': labels.sample(10).values})

# نمایش داده‌ها
print(sample_data)

import tensorflow as tf
from tensorflow.keras.layers import LSTM, Bidirectional, Conv1D, Dense, Dropout, Embedding, BatchNormalization, Flatten, Add
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.regularizers import l2

# تقسیم داده‌ها به train, val و test
X_train, X_temp, y_train, y_temp = train_test_split(X_shuffled, y_shuffled, test_size=0.3, random_state=180)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=180)

# توکن‌سازی و پردازش داده‌ها
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# تبدیل متون به توالی‌های عددی
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# پد کردن توالی‌ها
max_length = 1000
X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_val_pad = tf.keras.preprocessing.sequence.pad_sequences(X_val_seq, maxlen=max_length, padding='post')
X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# تبدیل برچسب‌ها به one-hot
y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=3)
y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=3)
y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=3)

# معماری مدل
input_layer = Input(shape=(max_length,))
embedding = Embedding(input_dim=20000, output_dim=128)(input_layer)

# بلوک LSTM
lstm_block = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)))(embedding)
lstm_block = BatchNormalization()(lstm_block)
lstm_block = Dropout(0.6)(lstm_block)  # افزایش نرخ Dropout
lstm_block = Bidirectional(LSTM(128, kernel_regularizer=l2(0.01)))(lstm_block)
lstm_block = Dense(64, activation='relu')(lstm_block)
lstm_block = Dropout(0.6)(lstm_block)  # افزایش نرخ Dropout

# بلوک CNN
cnn_block = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding)
cnn_block = BatchNormalization()(cnn_block)
cnn_block = Dropout(0.6)(cnn_block)  # افزایش نرخ Dropout
cnn_block = Flatten()(cnn_block)
cnn_block = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(cnn_block)

# اتصال بلوک‌ها
combined = Add()([lstm_block, cnn_block])
output_layer = Dense(3, activation='softmax')(combined)

# تعریف مدل
model = Model(inputs=input_layer, outputs=output_layer)

# کامپایل مدل
model.compile(
    loss='categorical_crossentropy',
    optimizer=Adam(learning_rate=1e-5),  # کاهش learning rate
    metrics=['accuracy']
)

# تعریف Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)  # کاهش patience
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)

# آموزش مدل
model.fit(
    X_train_pad, y_train_cat,
    epochs=15,
    validation_data=(X_val_pad, y_val_cat),
    callbacks=[early_stopping, reduce_lr]
)

# ارزیابی مدل
test_loss, test_accuracy = model.evaluate(X_test_pad, y_test_cat, verbose=1)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

import matplotlib.pyplot as plt
import numpy as np

# پیش‌بینی بر روی داده‌های تست
y_pred_prob = model.predict(X_test_pad)

# استخراج اعتماد پیش‌بینی‌ها (مقدار احتمال هر کلاس)
confidence_scores = np.max(y_pred_prob, axis=1)

# ترسیم نمودار توزیع اعتماد
plt.figure(figsize=(8, 6))
plt.hist(confidence_scores, bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Model Confidence')
plt.xlabel('Confidence Score')
plt.ylabel('Frequency')
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# پیش‌بینی کلاس‌ها (برچسب‌ها)
y_pred_classes = np.argmax(y_pred_prob, axis=1)

# محاسبه ماتریس سردرگمی
cm = confusion_matrix(np.argmax(y_test_cat, axis=1), y_pred_classes)

# ترسیم ماتریس سردرگمی
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# تبدیل برچسب‌ها به باینری
y_test_bin = label_binarize(np.argmax(y_test_cat, axis=1), classes=[0, 1, 2])
y_pred_prob_bin = y_pred_prob

# ترسیم نمودار ROC برای هر کلاس
plt.figure(figsize=(8, 6))
for i in range(3):  # 3 کلاس
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_prob_bin[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'Class {i} (AUC = {roc_auc:.2f})')

# اضافه کردن اطلاعات به نمودار
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

from sklearn.metrics import precision_recall_curve

# ترسیم نمودار Precision-Recall برای هر کلاس
plt.figure(figsize=(8, 6))
for i in range(3):  # 3 کلاس
    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_prob_bin[:, i])
    plt.plot(recall, precision, lw=2, label=f'Class {i}')

# اضافه کردن اطلاعات به نمودار
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend(loc='lower left')
plt.show()

test_sentences = [
    "نظام سیاسی کشور باید به گونه‌ای باشد که حقوق بشر در اولویت قرار گیرد و آزادی‌های فردی در آن به خوبی رعایت شود.",
    "به نظر می‌رسد که تصمیمات اقتصادی اخیر دولت تاثیرات منفی زیادی بر روی اقشار کم درآمد جامعه گذاشته است.",
    "باید به مسائل زیست‌محیطی بیشتر توجه شود تا نسل‌های آینده بتوانند از منابع طبیعی بهره‌مند شوند.",
    "در شرایط فعلی، سیاست‌های خارجی کشور باید به گونه‌ای بازنگری شوند که منافع ملی بیشتر از هر چیزی در نظر گرفته شود.",
    "انتخابات اخیر نشان‌دهنده تغییرات بنیادینی در نگرش مردم نسبت به سیاست‌های حاکم است.",
    "برخی از مخالفان دولت معتقدند که سیستم قضائی کشور به دلیل دخالت‌های سیاسی تحت فشار قرار دارد.",
    "تحریم‌های بین‌المللی تاثیر زیادی بر روی اقتصاد کشور گذاشته و مشکلات زیادی برای مردم ایجاد کرده است.",
    "آزادی بیان باید در تمام جوامع رعایت شود تا شهروندان بتوانند نظرات خود را بدون ترس از سرکوب بیان کنند.",
    "حکومت باید به نوعی عمل کند که فاصله طبقاتی در جامعه کاهش یابد و عدالت اجتماعی به درستی اجرا شود.",
    "در شرایط کنونی، جامعه جهانی باید برای مقابله با بحران‌های زیست‌محیطی همکاری بیشتری داشته باشد.",
    "سیاست‌های آموزش و پرورش باید به گونه‌ای تغییر کنند که نسل جوان بتوانند با چالش‌های جهانی مواجه شوند.",
    "مردم باید بیشتر از گذشته در روند تصمیم‌گیری‌های سیاسی مشارکت داشته باشند تا نظام دموکراتیک تقویت شود.",
    "پیشرفت در فناوری اطلاعات می‌تواند کشور را از نظر اقتصادی و سیاسی به جایگاه بالاتری در سطح جهانی برساند.",
    "در کشورهای در حال توسعه، نابرابری‌های اجتماعی یکی از بزرگترین چالش‌ها به شمار می‌آید که باید برای آن راه‌حل‌هایی پیدا شود.",
    "حاکمیت ملی و احترام به قوانین بین‌المللی دو اصل اساسی در روابط بین کشورها هستند که باید به آن‌ها توجه شود.",
    "اینکه کشورها در زمینه حقوق بشر پیشرفت کنند و به حقوق اقلیت‌ها احترام بگذارند، از اهمیت ویژه‌ای برخوردار است.",
    "سیاست‌های مالیاتی باید به گونه‌ای طراحی شوند که عدالت اجتماعی را در جامعه تقویت کنند و شکاف‌های اقتصادی را کاهش دهند.",
    "کشورهای مختلف باید برای مبارزه با تروریسم و افراط‌گرایی در سطح جهانی همکاری کنند و این مسئله نباید محدود به مرزهای خاصی باشد.",
    "شورای امنیت سازمان ملل باید به طور جدی‌تر به حل بحران‌های جهانی بپردازد و کشورها را از نقض حقوق بشر منع کند.",
    "دولت باید در راستای مقابله با فساد اداری گام‌های اساسی بردارد تا اعتماد مردم به سیستم حاکمیت دوباره بازگردد."
]

# توکن‌سازی جملات جدید
test_sequences = tokenizer.texts_to_sequences(test_sentences)

# پد کردن توالی‌ها
test_pad = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_length, padding='post')

# پیش‌بینی با مدل
predictions = model.predict(test_pad)

# تبدیل پیش‌بینی‌ها به برچسب‌ها
predicted_labels = tf.argmax(predictions, axis=1)

# چاپ نتایج
for sentence, label in zip(test_sentences, predicted_labels):
    print(f"جمله: '{sentence}' -> پیش‌بینی: {label.numpy()}")

# ذخیره مدل به صورت کامل (معماری + وزن‌ها)
model.save('sentiment_model.h5')

print("مدل با موفقیت ذخیره شد.")

# بارگذاری مدل ذخیره شده
loaded_model = tf.keras.models.load_model('sentiment_model.h5')

print("مدل با موفقیت بارگذاری شد.")