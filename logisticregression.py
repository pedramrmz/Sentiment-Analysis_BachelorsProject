# -*- coding: utf-8 -*-
"""LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wa3X2y7pw1SdvQN6D9yiSa9yzEutdGfF

**install library**
"""

!pip install hazm

!pip install urlextract

!pip install emojis

!pip install keras

!pip install keras

"""## import"""

from __future__ import unicode_literals
from google.colab import drive
from hazm import *
import tensorflow as tf
from keras.models import Sequential
import pandas as pd
from keras.layers import Dense
import numpy as np
import re
from urlextract import URLExtract
import emojis
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
#from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import Adadelta,Adam,RMSprop
#from keras.utils import np_utils
from tqdm import tqdm

from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.optimizers import Adadelta, Adam, RMSprop
from tensorflow.keras.utils import to_categorical

"""# **connect to drive**"""

drive.mount('/content/drive')

file_path = '/content/drive/My Drive/firstdata.csv'

corpus = pd.read_csv(file_path, on_bad_lines='skip', delimiter='\t')
corpus.head()

"""# preprocess dataset"""

import re
import emojis
from urlextract import URLExtract
import pandas as pd
from tqdm import tqdm

def preprocess(text):
    # تابع برای جایگزینی چندگانه
    def _multiple_replace(mapping, text):
        pattern = "|".join(map(re.escape, mapping.keys()))
        return re.sub(pattern, lambda m: mapping[m.group()], str(text))

    # تبدیل اعداد فارسی به انگلیسی
    def convert_fa_numbers(input_str):
        mapping = {
            '۰': '0', '۱': '1', '۲': '2', '۳': '3', '۴': '4',
            '۵': '5', '۶': '6', '۷': '7', '۸': '8', '۹': '9',
        }
        return _multiple_replace(mapping, input_str)

    # تبدیل حروف عربی به فارسی
    def convert_ar_characters(input_str):
        mapping = {
            'ك': 'ک', 'ى': 'ی', 'ي': 'ی', 'ئ': 'ی', 'إ': 'ا',
            'أ': 'ا', 'ة': 'ه', 'ؤ': 'و'
        }
        return _multiple_replace(mapping, input_str)

    # حذف اموجی‌های تکراری
    def removeEmojies(text):
        e = ("["
              u"\U0001F600-\U0001F64F"  # emoticons
              u"\U0001F300-\U0001F5FF"  # symbols & pictographs
              u"\U0001F680-\U0001F6FF"  # transport & map symbols
              u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
              u"\U00002500-\U00002BEF"  # chinese char
              u"\U00002702-\U000027B0"
              u"\U00002702-\U000027B0"
              u"\U000024C2-\U0001F251"
              u"\U0001f926-\U0001f937"
              u"\U00010000-\U0010ffff"
              u"\u2640-\u2642"
              u"\u2600-\u2B55"
              u"\u200d"
              u"\u23cf"
              u"\u23e9"
              u"\u231a"
              u"\ufe0f"  # dingbats
              u"\u3030"
              "]+")

        duplemoj = re.compile('(' + e + '){2,}|((' + e + r')\s+)' '{2,}', re.UNICODE)
        return duplemoj.sub('', text)

    # نرمال‌سازی متن برای حذف نیم‌فاصله‌ها و حرکات عربی
    def normalize(text):
        text = re.sub(r'[‌ـ]', '', text)  # حذف نیم‌فاصله و کشیدگی
        text = re.sub(r'[\u064B-\u0652]', '', text)  # حذف حرکات عربی
        return text

    # استخراج و جایگزینی URL‌ها
    extractor = URLExtract()
    for url in extractor.gen_urls(text):
        text = text.replace(url, '<URL>')

    # جایگزینی اموجی‌ها با <emoji>
    emj = emojis.get(text)
    for i in emj:
        if i in text:
            text = text.replace(i, '<emoji>')

    # تبدیل اعداد فارسی به انگلیسی
    text = convert_fa_numbers(text)

    # تبدیل حروف عربی به معادل فارسی
    text = convert_ar_characters(text)

    # حذف اموجی‌های تکراری
    text = removeEmojies(text)

    # جایگزینی اسمایل‌ها با <smiley>
    text = re.sub(r"(:\)|:-\)|:\(|:-\()", '<smiley>', text)

    # تبدیل متن به حروف کوچک
    text = text.lower()

    # نرمال‌سازی متن برای حذف نیم‌فاصله‌ها
    text = normalize(text)

    # حذف علائم نگارشی و کاراکترهای اضافی
    text = re.sub(r'[<>#.:()"\'!?؟،؛,@$%^&*_+\[\]/]', ' ', text)
    text = re.sub(r'[\s]{2,}', ' ', text)
    text = re.sub(r'(\w)\1{2,}', r'\1', text)

    # حذف استاپ‌وردهای فارسی
    stopwords = set(['و', 'به', 'از', 'که', 'را', 'این', 'آن', 'با', 'برای', 'در', 'تا'])
    text = ' '.join([word for word in text.split() if word not in stopwords])

    # بررسی اینکه آیا متن شامل حروف فارسی است
    if re.search(r'[\u0600-\u06FF]', text):
        return text.strip()  # حذف فضاهای اضافی
    else:
        return ''  # به جای 'None'، یک رشته خالی برمی‌گردانیم

tqdm.pandas()

#corpus['Cleaned'] = corpus['comment'].progress_apply(preprocess)
corpus_first_200 = corpus.head(7000).copy()
#corpus_first_200 = corpus.head(200)
corpus_first_200['Cleaned'] = corpus_first_200['comment'].progress_apply(preprocess)

corpus_first_200 = corpus_first_200.drop('Unnamed: 0', axis=1)

corpus_first_200.head()

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=corpus_first_200)

corpus_first_200.to_csv('myclaenData.csv')

count_vectorizer = CountVectorizer()
X_count_vectorized = count_vectorizer.fit_transform(corpus_first_200.Cleaned).todense()

vectorizer = TfidfVectorizer(min_df=2, max_features= 10000)
X_tfidf_vectorized = vectorizer.fit_transform(corpus_first_200.Cleaned).todense()
labels = corpus_first_200['label_id'].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_count_vectorized, labels, test_size=0.2, random_state=42)
X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(X_tfidf_vectorized, labels, test_size=0.2, random_state=42)
input_dim = X_tfidf_train.shape
print(input_dim)

#مدیریت مقادیر گم شده
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='most_frequent')  # یا 'mean', 'median', و غیره
y_train_cleaned = imputer.fit_transform(y_train.reshape(-1, 1)).ravel()

import numpy as np

# شمارش مقادیر NaN در y_train
nan_count_y = np.isnan(y_train).sum()
print("Number of NaN values in y_train:", nan_count_y)

if nan_count_y > 0:
    # ایجاد یک ماسک برای مقادیر غیر NaN در y_train
    mask_y = ~np.isnan(y_train)

    # فیلتر کردن X_train و y_train با استفاده از ماسک
    X_train_clean = X_train[mask_y]
    y_train_clean = y_train[mask_y]

    # بررسی تعداد مقادیر NaN بعد از فیلتر
    print("Number of NaN values in y_train after cleaning:", np.isnan(y_train_clean).sum())
else:
    X_train_clean = X_train
    y_train_clean = y_train

import numpy as np
from sklearn.linear_model import LogisticRegression

# Convert to NumPy arrays if they are matrices
X_train_clean = np.asarray(X_train_clean)
y_train_clean = np.asarray(y_train_clean)

# Fit the classifier
classifier = LogisticRegression()
classifier.fit(X_train_clean, y_train_clean)

# Calculate and print accuracy
score = classifier.score(X_train_clean, y_train_clean)
print("Accuracy:", score)

import numpy as np
X_tfidf_train = np.nan_to_num(X_tfidf_train)
y_tfidf_train = np.nan_to_num(y_tfidf_train)
# Filter out rows with NaN in y
valid_indices_train = ~np.isnan(y_tfidf_train)
X_tfidf_train_clean = X_tfidf_train[valid_indices_train]
y_tfidf_train_clean = y_tfidf_train[valid_indices_train]

valid_indices_test = ~np.isnan(y_tfidf_test)
X_tfidf_test_clean = X_tfidf_test[valid_indices_test]
y_tfidf_test_clean = y_tfidf_test[valid_indices_test]

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='most_frequent')  # or use 'mean'/'median' for numerical values
y_tfidf_train_imputed = imputer.fit_transform(y_tfidf_train.reshape(-1, 1)).ravel()
y_tfidf_test_imputed = imputer.transform(y_tfidf_test.reshape(-1, 1)).ravel()

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Ensure your feature matrices and target vectors are in array format
X_tfidf_train_clean = np.asarray(X_tfidf_train_clean)  # Convert to array
X_tfidf_test_clean = np.asarray(X_tfidf_test_clean)    # Convert to array

y_tfidf_train_clean = np.asarray(y_tfidf_train_clean)  # Convert to array
y_tfidf_test_clean = np.asarray(y_tfidf_test_clean)    # Convert to array

# Optional: Handle NaN values in y if necessary
if np.any(np.isnan(y_tfidf_train_clean)):
    # Imputation or filtering logic can be applied here, e.g.:
    # y_tfidf_train_clean = np.nan_to_num(y_tfidf_train_clean)  # Replace NaNs with 0
    print("NaN values found in y_tfidf_train_clean")

if np.any(np.isnan(y_tfidf_test_clean)):
    # Imputation logic for the test set, e.g.:
    # y_tfidf_test_clean = np.nan_to_num(y_tfidf_test_clean)  # Replace NaNs with 0
    print("NaN values found in y_tfidf_test_clean")

# Initialize and fit the classifier
clf = LogisticRegression()
clf.fit(X_tfidf_train_clean, y_tfidf_train_clean)

# Calculate and print accuracy
tfidf_score = clf.score(X_tfidf_test_clean, y_tfidf_test_clean)
print("Accuracy:", tfidf_score)

# Optional: Get predictions and a classification report
y_pred = clf.predict(X_tfidf_test_clean)
print(classification_report(y_tfidf_test_clean, y_pred))

nb_classes = 2
batch_size = 32
nb_epochs = 10

import numpy as np
from keras.utils import to_categorical  # or from tensorflow.keras.utils import to_categorical

# Assuming y_tfidf_train_clean is already defined
y_tfidf_train_cat = to_categorical(y_tfidf_train_clean)

model = Sequential()

model.add(Dense(1000,input_shape= (input_dim[1],)))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(500))

model.add(Activation('relu'))

model.add(Dropout(0.5))

model.add(Dense(50))

model.add(Activation('relu'))

model.add(Dropout(0.5))

model.add(Dense(nb_classes))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam')
tf.config.run_functions_eagerly(True)
model.fit(X_tfidf_train, y_tfidf_train_cat, batch_size=batch_size, epochs=nb_epochs,verbose=2)

y_test_pred = model.predict(X_tfidf_test_clean)
y_test_predclass = np.argmax(y_test_pred, axis=1)
y_trian_pred = model.predict(X_tfidf_train_clean)
y_train_predclass = np.argmax(y_trian_pred, axis=1)

#این با شکبه عصبی نوشتم که به درد نمیخورد
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Check shapes of true labels and predicted labels
print("Shapes of arrays:")
print("y_tfidf_test shape:", y_tfidf_test.shape)         # True labels for test set
print("y_test_predclass shape:", y_test_predclass.shape)   # Predicted labels for test set
print("y_tfidf_train shape:", y_tfidf_train.shape)         # True labels for train set
print("y_train_predclass shape:", y_train_predclass.shape)   # Predicted labels for train set

# Function to remove NaN values and ensure consistent lengths
def remove_nan_and_trim(y_true, y_pred):
    # Determine the minimum length to avoid broadcasting issues
    min_length = min(len(y_true), len(y_pred))

    # Trim both arrays to the minimum length
    y_true_trimmed = y_true[:min_length]
    y_pred_trimmed = y_pred[:min_length]

    # Create a mask to filter out NaNs
    mask = ~np.isnan(y_true_trimmed) & ~np.isnan(y_pred_trimmed)
    return y_true_trimmed[mask], y_pred_trimmed[mask]

# Ensure consistent lengths for test set
y_tfidf_test_clean, y_test_predclass_clean = remove_nan_and_trim(y_tfidf_test, y_test_predclass)

# Calculate accuracy if lengths match
if y_tfidf_test_clean.shape[0] == y_test_predclass_clean.shape[0]:
    test_accuracy = round(accuracy_score(y_tfidf_test_clean, y_test_predclass_clean), 4) * 100
    print("nDeep Neural Network - Test accuracy:", test_accuracy)
else:
    print("Mismatch in test labels and predictions after cleaning!")

# Ensure consistent lengths for train set
if y_tfidf_train.shape[0] == y_train_predclass.shape[0]:
    train_accuracy = round(accuracy_score(y_tfidf_train, y_train_predclass), 4) * 100
    print("nDeep Neural Network - Train accuracy:", train_accuracy)
else:
    print("Mismatch in train labels and predictions!")

#اگه بالایی رو ران کردی این رو ران نکن
from sklearn.metrics import accuracy_score,classification_report
print ("nDeep Neural Network - Test accuracy:",(round(accuracy_score(y_tfidf_test, y_test_predclass),4)*100))
print ("nDeep Neural Network - Train accuracy:",(round(accuracy_score(y_tfidf_train, y_train_predclass),4)*100))

X_pred = vectorizer.transform([preprocess('دیگه از این رستوران سفارش نمیدم')]).todense()
model.predict(X_pred)