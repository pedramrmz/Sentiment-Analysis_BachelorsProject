# -*- coding: utf-8 -*-
"""lstm

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZCl0eCa2xMCmACxnIhLGtAaxsf9igpVF
"""

!pip install tensorflow

!pip install hazm
!pip install urlextract
!pip install emojis
!pip install keras

# اتصال به گوگل درایو و بارگذاری داده‌ها
from google.colab import drive
import pandas as pd
import re
import emojis
from urlextract import URLExtract
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from tqdm import tqdm

# اتصال به گوگل درایو
drive.mount('/content/drive')
file_path = '/content/drive/My Drive/firstdata.csv'

# بارگذاری داده‌ها
corpus = pd.read_csv(file_path, on_bad_lines='skip', delimiter='\t').head(7000)  # انتخاب 700 ردیف اول
corpus.head()

# تابع پیش‌پردازش
def preprocess(text):
    def _multiple_replace(mapping, text):
        pattern = "|".join(map(re.escape, mapping.keys()))
        return re.sub(pattern, lambda m: mapping[m.group()], str(text))

    def convert_fa_numbers(input_str):
        mapping = {
            '۰': '0', '۱': '1', '۲': '2', '۳': '3', '۴': '4',
            '۵': '5', '۶': '6', '۷': '7', '۸': '8', '۹': '9',
        }
        return _multiple_replace(mapping, input_str)

    def convert_ar_characters(input_str):
        mapping = {
            'ك': 'ک', 'ى': 'ی', 'ي': 'ی', 'ئ': 'ی', 'إ': 'ا',
            'أ': 'ا', 'ة': 'ه', 'ؤ': 'و'
        }
        return _multiple_replace(mapping, input_str)

    def removeEmojies(text):
        e = ("["
              u"\U0001F600-\U0001F64F"
              u"\U0001F300-\U0001F5FF"
              u"\U0001F680-\U0001F6FF"
              u"\U0001F1E0-\U0001F1FF"
              u"\U00002500-\U00002BEF"
              u"\U00002702-\U000027B0"
              u"\U000024C2-\U0001F251"
              u"\U0001f926-\U0001f937"
              u"\U00010000-\U0010ffff"
              u"\u2640-\u2642"
              u"\u2600-\u2B55"
              u"\u200d"
              u"\u23cf"
              u"\u23e9"
              u"\u231a"
              u"\ufe0f"
              u"\u3030"
              "]+")
        duplemoj = re.compile('(' + e + '){2,}|((' + e + r')\s+)' '{2,}', re.UNICODE)
        return duplemoj.sub('', text)

    def normalize(text):
        text = re.sub(r'[‌ـ]', '', text)
        text = re.sub(r'[\u064B-\u0652]', '', text)
        return text

    extractor = URLExtract()
    for url in extractor.gen_urls(text):
        text = text.replace(url, '<URL>')

    emj = emojis.get(text)
    for i in emj:
        if i in text:
            text = text.replace(i, '<emoji>')

    text = convert_fa_numbers(text)
    text = convert_ar_characters(text)
    text = removeEmojies(text)
    text = re.sub(r"(:\)|:-\)|:\(|:-\()", '<smiley>', text)
    text = text.lower()
    text = normalize(text)
    text = re.sub(r'[<>#.:()"\'!?؟،؛,@$%^&*_+\[\]/]', ' ', text)
    text = re.sub(r'[\s]{2,}', ' ', text)
    text = re.sub(r'(\w)\1{2,}', r'\1', text)

    stopwords = set(['و', 'به', 'از', 'که', 'را', 'این', 'آن', 'با', 'برای', 'در', 'تا'])
    text = ' '.join([word for word in text.split() if word not in stopwords])

    if re.search(r'[\u0600-\u06FF]', text):
        return text.strip()
    else:
        return ''

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences  # وارد کردن pad_sequences
from keras.callbacks import EarlyStopping
from keras.layers import BatchNormalization
# اعمال پیش‌پردازش به داده‌ها
corpus['comment'] = corpus['comment'].apply(preprocess)

# آماده‌سازی ورودی‌ها و برچسب‌ها
X = corpus['comment']
labels = corpus['label_id']

# حذف مقادیر گم‌شده
X = X[labels.notnull()]
labels = labels.dropna()


# تبدیل برچسب‌ها به قالب صحیح
labels = labels.astype(int)

# تقسیم داده‌ها به مجموعه‌های آموزش و آزمون
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
# توکن‌سازی متون
tokenizer = Tokenizer(num_words=20000)
tokenizer.fit_on_texts(X_train)
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# پد کردن توالی‌ها
max_length = 1000  # حداکثر طول جملات
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# تبدیل برچسب‌ها به قالب one-hot
y_train_cat = to_categorical(y_train, num_classes=2)
y_test_cat = to_categorical(y_test, num_classes=2)






# افزایش ابعاد لایه‌های Embedding و LSTM
rnn_model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=20000, output_dim=64, input_length=max_length),
    tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),
    BatchNormalization(),
    tf.keras.layers.Dropout(0.4),  # تغییر نرخ Dropout
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),
    BatchNormalization(),
    tf.keras.layers.Dropout(0.4),  # تغییر نرخ Dropout
    tf.keras.layers.Dense(2, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))  # اضافه کردن Regularization
])

# کامپایل مدل با نرخ یادگیری جدید
rnn_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])

# ایجاد EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3)

# آموزش مدل
rnn_model.fit(X_train_pad, y_train_cat, epochs=30, validation_data=(X_test_pad, y_test_cat), verbose=1, callbacks=[early_stopping])